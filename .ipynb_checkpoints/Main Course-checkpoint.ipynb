{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stable-baselines3.readthedocs.io/en/master/guide/rl.html\n",
    "# https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#a-taxonomy-of-rl-algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in c:\\users\\andy\\anaconda3\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\andy\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\andy\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.21.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\andy\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (3.5.2)\n",
      "Requirement already satisfied: torch>=1.11 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\andy\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.4.4)\n",
      "Requirement already satisfied: gym==0.21 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.21.0)\n",
      "Requirement already satisfied: importlib-metadata~=4.13 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.13.0)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.6.0 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.13.0)\n",
      "Requirement already satisfied: rich in c:\\users\\andy\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (13.3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\andy\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.64.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\andy\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.7.0.72)\n",
      "Requirement already satisfied: pillow in c:\\users\\andy\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (9.2.0)\n",
      "Requirement already satisfied: ale-py==0.7.4 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.7.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\andy\\appdata\\roaming\\python\\python39\\site-packages (from stable-baselines3[extra]) (5.9.5)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\andy\\anaconda3\\lib\\site-packages (from ale-py==0.7.4->stable-baselines3[extra]) (5.12.0)\n",
      "Requirement already satisfied: click in c:\\users\\andy\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (8.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\andy\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (2.28.1)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\andy\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\andy\\appdata\\roaming\\python\\python39\\site-packages (from importlib-metadata~=4.13->stable-baselines3[extra]) (3.15.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.54.0)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.23.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.0.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (63.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.18.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\andy\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3[extra]) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\andy\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.11->stable-baselines3[extra]) (4.5.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3[extra]) (2.11.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\andy\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3[extra]) (2.8.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\andy\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3[extra]) (1.10.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andy\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->stable-baselines3[extra]) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\andy\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2022.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\andy\\appdata\\roaming\\python\\python39\\site-packages (from rich->stable-baselines3[extra]) (2.15.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\andy\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->stable-baselines3[extra]) (0.4.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (1.26.11)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\andy\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.6.0->stable-baselines3[extra]) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11->stable-baselines3[extra]) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11->stable-baselines3[extra]) (1.2.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\andy\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyglet==1.5.27 in c:\\users\\andy\\anaconda3\\lib\\site-packages (1.5.27)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3[extra]\n",
    "!pip install pyglet==1.5.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from stable_baselines3 import PPO #ALGORITHM\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv #this allows vectorization which allows multiple training at the same time\n",
    "from stable_baselines3.common.evaluation import evaluate_policy #makes it easier to test how the environment is performence, shows standard deviation and reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated vs Real Environments\n",
    "\n",
    "It is important to consider the agent, policies, and environment.\n",
    "The environment can be either real or simulated. Simulated environments give you the ability to trial and train a model in a safe AND cost effective manner\n",
    "\n",
    "## OpenAI Gym \n",
    "Gives an easy way to build environments for training RL agents\n",
    "\n",
    "When we look at OpenAI Gym environment, they are represented by Spaces\n",
    "\n",
    "Box - n dimensional tensor, range of values. E.g. Box (0,1,shape = (3,3))\n",
    "(low value, high value, shape of the space)\n",
    "\n",
    "Discreate - set of items\n",
    "E.g. Discrete (3) - gives 0, 1, 2. Typically associated with actions\n",
    "\n",
    "Tuples - Tuple of other spaces e.g.Box or Discrete\n",
    "E.g. Tuple ((Discrete(2), Box(0,100,shape = (1,))))\n",
    "\n",
    "Dict - Dictionary of spaces e.g. Box or Discrete\n",
    "E.g. Dict ({'height': Discrete(2), \"speed\":Box(0,100, shape =(1,))})\n",
    "\n",
    "MultiBinary - One hot encoded binary values\n",
    "E.g. MultiBinary(4) list of values of 4 positions (0,1,2,3) with 0 or 1 located in each value\n",
    "\n",
    "MultiDiscrete - multiple discrete values\n",
    "E.g. MultiDiscrete ([5,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maps to the preinstalled OpenAI Gym Environment\n",
    "environment_name = \"CartPole-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:15.0\n",
      "Episode:2 Score:11.0\n",
      "Episode:3 Score:23.0\n",
      "Episode:4 Score:13.0\n",
      "Episode:5 Score:63.0\n"
     ]
    }
   ],
   "source": [
    "#this loop is used to test out the environment\n",
    "\n",
    "episodes = 5 # we are looping through the environment 5 times\n",
    "for episode in range(1, episodes+1): #loop each episode from 1 to 5\n",
    "    state = env.reset() #reset the environment, gives initial set of environment. We will later pass this to the agent to determine best path\n",
    "    done = False #temp variable on wheather or not it is done\n",
    "    score = 0  # score vairable\n",
    "    \n",
    "    while not done:\n",
    "        env.render() #this renders the environment for view\n",
    "        action = env.action_space.sample() #generate a random action\n",
    "        n_state, reward, done, info = env.step(action) #pass random action to environment\n",
    "        score+=reward #accumulate score\n",
    "    print('Episode:{} Score:{}'.format(episode, score)) #prints\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1)# returns next set of observation, reward (increment or decrement), if our episode is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space #this gives out 2 becuase there is only 2 actions the agent can take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding The Environment\n",
    "https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-push cart to left, 1-push cart to the right\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [cart position, cart velocity, pole angle, pole angular velocity]\n",
    "env.observation_space.sample()# cart position, velocity, pole angle, pole velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train an RL Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Algorithms\n",
    "\n",
    "Model Free RL - only uses current based value for predication\n",
    "\n",
    "Model Based RL - make a predication of the future state of the model\n",
    "\n",
    "Check out: https://spinningup.openai.com/en/latest/spinningup/\n",
    "\n",
    "The algoirthm used should map properly to what action space you need. Certain algorithm can only work on certain action spaces.\n",
    "\n",
    "For this tutorial, we know the action space is discrete so we can only use discrete algorithms\n",
    "\n",
    "## Training Metric\n",
    "\n",
    "we need to understand certain metrics\n",
    "\n",
    "Rollout Metrics:\n",
    "\n",
    "Episode length is how long the episode went for\n",
    "\n",
    "Episode reward is reward\n",
    "\n",
    "Time Metrics:\n",
    "\n",
    "fps, iterations, time_elapsed, total_timesteps\n",
    "\n",
    "Train:\n",
    "\n",
    "entropy_loss, explained_variance, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name) #recreated env\n",
    "env = DummyVecEnv([lambda: env]) #wrapped env into non vectorized env\n",
    "model = PPO('MlpPolicy', env, verbose = 1) #PPO is the algorithm, first input is policy (MlpPolicy is standard neural network layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000) #this is to train the model. Input is how long to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_model') # saved into the taining folder then saved models folder then save it as PPO_model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_path) #save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model# delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('PPO_model', env=env)# obtain the model and reload it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric is based on the algorithm itself \n",
    "\n",
    "PPO is considered solved if the model gives 200 or higher. Some environment has cap in which it is considered solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True) #test out environment policy, passes model, env, how many episodes, if you want to render it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset() #taking observation and passing it\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done: \n",
    "        print('info', info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Viewing Logs in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Adding a callback to the training Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training', 'Saved Models')\n",
    "log_path = os.path.join('Training', 'Logs') #make directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=190, verbose=1)\n",
    "eval_callback = EvalCallback(env, \n",
    "                             callback_on_new_best=stop_callback, \n",
    "                             eval_freq=10000, \n",
    "                             best_model_save_path=save_path, \n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('Training', 'Saved Models', 'best_model')\n",
    "model = PPO.load(model_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Changing Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch=[dict(pi=[128, 128, 128, 128], vf=[128, 128, 128, 128])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, policy_kwargs={'net_arch': net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Using an Alternate Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_path = os.path.join('Training', 'Saved Models', 'DQN_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(dqn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN.load(dqn_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
